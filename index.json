[{"authors":["babak"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"477f320c1a5a8817f51ddab82b2be868","permalink":"https://mea-lab.github.io/author/babak-taati/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/babak-taati/","section":"authors","summary":"","tags":null,"title":"Babak Taati","type":"authors"},{"authors":["diego"],"categories":null,"content":"Diego L. Guarin is an Assistant Professor in the Biomedical Engineering program at the Florida Institue of Technology. His research interests include the application of machine learning and computer vision for assessing neurological diseases, and the development of novel signal processing approaches to study the human neuromuscular system. He is the director of the Movement Evaluation and Analysis Laboratory at Florida Tech.\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6e05be09ebac4dbd9407c571fdc9dbe7","permalink":"https://mea-lab.github.io/author/diego-l.-guarin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/diego-l.-guarin/","section":"authors","summary":"Diego L. Guarin is an Assistant Professor in the Biomedical Engineering program at the Florida Institue of Technology. His research interests include the application of machine learning and computer vision for assessing neurological diseases, and the development of novel signal processing approaches to study the human neuromuscular system.","tags":null,"title":"Diego L. Guarin","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://mea-lab.github.io/author/nelson-bighetti/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nelson-bighetti/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"e31504cb83947d1dff041ce0f1052419","permalink":"https://mea-lab.github.io/author/nelson-bighettiii/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/nelson-bighettiii/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighettiii","type":"authors"},{"authors":["rob"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"17f79dadea841522d6c2fc0cef50c112","permalink":"https://mea-lab.github.io/author/robert-e.-kearney/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/robert-e.-kearney/","section":"authors","summary":"","tags":null,"title":"Robert E. Kearney","type":"authors"},{"authors":["Tessa"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"517d18ec21971f101184f5abc6bfa6b7","permalink":"https://mea-lab.github.io/author/tessa-a.-hadlock/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/tessa-a.-hadlock/","section":"authors","summary":"","tags":null,"title":"Tessa A. Hadlock","type":"authors"},{"authors":["Yana"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4d0b9b7ac27376bff00d7271c0c034ea","permalink":"https://mea-lab.github.io/author/yana-yunusova/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/yana-yunusova/","section":"authors","summary":"","tags":null,"title":"Yana Yunusova","type":"authors"},{"authors":[],"categories":[],"content":"","date":1589514050,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589514050,"objectID":"c636e8036903b3af3febf6ab1875a784","permalink":"https://mea-lab.github.io/project/my-project-name/","publishdate":"2020-05-14T23:40:50-04:00","relpermalink":"/project/my-project-name/","section":"project","summary":"","tags":[],"title":"My Project Name","type":"project"},{"authors":[],"categories":[],"content":"","date":1589513425,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589513425,"objectID":"fe859c6266641600992fd6a0d6f11947","permalink":"https://mea-lab.github.io/post/post1/","publishdate":"2020-05-14T23:30:25-04:00","relpermalink":"/post/post1/","section":"post","summary":"","tags":[],"title":"post","type":"post"},{"authors":["Diego L. Guarin","Aidan Dempster","Andrea Bandini","Yana Yunusova","Babak Taati"],"categories":["Facial Analysis"],"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"e8d990215beb6a0da1b8bbb652217dbf","permalink":"https://mea-lab.github.io/publication/2020e/","publishdate":"1969-12-31T19:33:40-05:00","relpermalink":"/publication/2020e/","section":"publication","summary":"","tags":["Facial Analysis"],"title":"Estimation of Orofacial Kinematics in Parkinson's Disease: Comparison of 2D and 3D Markerless Systems for Motion Tracking","type":"publication"},{"authors":["Diego L. Guarin","Babak Taati","Tessa A. Hadlock","Yana Yunusova"],"categories":["Facial Analysis"],"content":"","date":1583020800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583020800,"objectID":"76195a19bcd5fe72450ffd93e488b981","permalink":"https://mea-lab.github.io/publication/2020a/","publishdate":"1969-12-31T19:33:40-05:00","relpermalink":"/publication/2020a/","section":"publication","summary":" **Background** Automatic facial landmark localization in videos is an important first step in many computer vision applications, including the objective assessment of orofacial function. Convolutional neural networks (CNN) for facial landmarks localization are typically trained on faces of healthy and young adults, so model performance is inferior when applied to faces of older adults or people with diseases that affect facial movements, a phenomenon known as algorithmic bias. Fine-tuning pre-trained CNN models with representative data is a well-known technique used to reduce algorithmic bias and improve performance on clinical populations. However, the question of how much data is needed to properly fine-tune the model remains. **Methods** In this paper, we fine-tuned a popular CNN model for automatic facial landmarks localization using different number of manually annotated photographs from patients with facial palsy and evaluated the effects of the number of photographs used for model fine-tuning in the model performance by computing the normalized root mean squared error between the facial landmarks positions predicted by the model and those provided by manual annotators. Furthermore, we studied the effect of annotator bias by fine-tuning and evaluating the model with data provided by multiple annotators. **Results** Our results showed that fine-tuning the model with as little as 8 photographs from a single patient significantly improved the model performance on other individuals from the same clinical population, and that the best performance was achieved by fine-tuning the model with 320 photographs from 40 patients. Using more photographs for fine-tuning did not improve the model performance further. Regarding the annotator bias, we found that fine-tuning a CNN model with data from one annotator resulted in models biased against other annotators; our results also showed that this effect can be diminished by averaging data from multiple annotators. **Conclusions** It is possible to remove the algorithmic bias of a\textbf{depth} CNN model for automatic facial landmark localization using data from only 40 participants (total of 320 photographs). These results pave the way to future clinical applications of CNN models for the automatic assessment of orofacial function in different clinical populations, including patients with Parkinson’s disease and stroke.","tags":["Facial Analysis","Fairnes in AI","Facial Palsy","Deep-learning"],"title":"Automatic Facial Landmark Localization in Clinical Populations--Improving Model Performance with a Small Dataset","type":"publication"},{"authors":["Diego L. Guarin","Yana Yunusova","Babak Taati","Joseph R. Dusseldorp","Suresh Mohan","Joana Tavares","Martinus M. van Veen","Emily Fortier","Tessa A. Hadlock","and Nate Jowett"],"categories":["Facial Analysis"],"content":"","date":1580515200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580515200,"objectID":"301a001bed99ddd0817b5ec1757eb4f9","permalink":"https://mea-lab.github.io/publication/2020b/","publishdate":"1969-12-31T19:33:40-05:00","relpermalink":"/publication/2020b/","section":"publication","summary":" **Importance** Quantitative assessment of facial function is challenging, and subjective grading scales such as House–Brackmann, Sunnybrook, and eFACE have well-recognized limitations. Machine learning (ML) approaches to facial landmark localization carry great clinical potential as they enable high-throughput automated quantification of relevant facial metrics from photographs and videos. However, the translation from research settings to clinical application still requires important improvements. **Objective** To develop a novel ML algorithm for fast and accurate localization of facial landmarks in photographs of facial palsy patients and utilize this technology as part of an automated computer-aided diagnosis system. **Design, Setting, and Participants** Portrait photographs of 8 expressions obtained from 200 facial palsy patients and 10 healthy participants were manually annotated by localizing 68 facial landmarks in each photograph and by 3 trained clinicians using a custom graphical user interface. A novel ML model for automated facial landmark localization was trained using this disease-specific database. Algorithm accuracy was compared with manual markings and the output of a model trained using a larger database consisting only of healthy subjects. **Main Outcomes and Measurements** Root mean square error normalized by the interocular distance (NRMSE) of facial landmark localization between prediction of ML algorithm and manually localized landmarks. **Results**\nPublicly available algorithms for facial landmark localization provide poor localization accuracy when applied to photographs of patients compared with photographs of healthy controls (NRMSE, 8.56 ± 2.16 vs. 7.09 ± 2.34, p ≪ 0.01). We found significant improvement in facial landmark localization accuracy for the facial palsy patient population when using a model trained with a relatively small number photographs (1440) of patients compared with a model trained using several thousand more images of healthy faces (NRMSE, 6.03 ± 2.43 vs. 8.56 ± 2.16, p ≪ 0.01). **Conclusions and Relevance**\nRetraining a computer vision facial landmark detection model with fewer than 1600 annotated images of patients significantly improved landmark detection performance in frontal view photographs of this population. The new annotated database and facial landmark localization model represent the first steps toward an automatic system for computer-aided assessment in facial palsy. ","tags":["Facial Analysis","Computer-based diagnosis","Facial Palsy"],"title":"Toward an automatic system for computer-aided assessment in facial palsy","type":"publication"},{"authors":["Ronit Malka","Diego L. Guarin","Suresh Mohan","Ivan Coto Hernandez","Pavel Gorelik","Ofer Mazor","Tessa A. Hadlock","and Nate Jowett"],"categories":["Rodent Model"],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2c25f93bd5d79bd1d81575160538dd26","permalink":"https://mea-lab.github.io/publication/2020c/","publishdate":"1969-12-31T19:33:40-05:00","relpermalink":"/publication/2020c/","section":"publication","summary":" **Background**\nDisease processes causing increased neural compartment pressure may induce transient or permanent neural dysfunction. Surgical decompression can prevent and reverse such nerve damage. Owing to insufficient evidence from controlled studies, the efficacy and optimal timing of decompression surgery remains poorly characterized for several entrapment syndromes. **New method**\nWe describe the design, manufacture, and validation of a device for study of entrapment neuropathy in a small animal model. This device applies graded extrinsic pressure to a peripheral nerve and wirelessly transmits applied pressure levels in real-time. We implanted the device in rats applying low (under 100 mmHg), intermediate (200–300 mmHg) and high (above 300 mmHg) pressures to induce entrapment neuropathy of the facial nerve to mimic Bell’s palsy. Facial nerve function was quantitatively assessed by tracking whisker displacements before, during, and after compression. **Results**\nAt low pressure, no functional loss was observed. At intermediate pressure, partial functional loss developed with return of normal function several days after decompression. High pressure demonstrated complete functional loss with incomplete recovery following decompression. Histology demonstrated uninjured, Sunderland grade III, and Sunderland grade V injury in nerves exposed to low, medium, and high pressure, respectively. **Comparison with existing methods**\nExisting animal models of entrapment neuropathy are limited by inability to measure and titrate applied pressure over time. **Conclusions**\nDescribed is a miniaturized, wireless, fully implantable device for study of entrapment neuropathy in a murine model, which may be broadly employed to induce various degrees of neural dysfunction and functional recovery in live animal models.","tags":["Facial Palsy","Rodent Model"],"title":"Implantable wireless device for study of entrapment neuropathy","type":"publication"},{"authors":["Jacqueline J. Greene","Diego L. Guarin","Joana Tavares","Emily Fortier","Mara Robinson","Joseph R. Dusseldorp","Olivia Quatela","Nate Jowett","Tessa A. Hadlock"],"categories":["Facial Analysis"],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"50dd133dab2e07461227c7174f25405f","permalink":"https://mea-lab.github.io/publication/2020d/","publishdate":"1969-12-31T19:33:40-05:00","relpermalink":"/publication/2020d/","section":"publication","summary":" **Objectives**\nFacial palsy causes variable facial disfigurement ranging from subtle asymmetry to crippling deformity. There is no existing standard database to serve as a resource for facial palsy education and research. We present a standardized set of facial photographs and videos representing the entire spectrum of flaccid and nonflaccid (aberrantly regenerated or synkinetic) facial palsy. To demonstrate the utility of the dataset, we describe the relationship between level of facial function and perceived emotion expression as determined by an automated emotion detection, machine learning‐based algorithm. **Methods**\nPhotographs and videos of patients with both flaccid and nonflaccid facial palsy were prospectively gathered. The degree of facial palsy was quantified using eFACE, House‐Brackmann, and Sunnybrook scales. Perceived emotion during a standard video of facial movements was determined using an automated, machine learning algorithm. **Results**\nSixty participants were enrolled and categorized by eFACE score across the range of facial function. Patients with complete flaccid facial palsy (eFACE ","tags":["Facial Analysis","Facial Palsy"],"title":"The spectrum of facial palsy: The MEEI facial palsy photo and video standard set","type":"publication"},{"authors":["Joseph R. Dusseldorp","Martinus M. van Veen","Diego L. Guarin","Olivia Quatela","Nate Jowett, and","Tessa A. Hadlock"],"categories":["Facial Analysis"],"content":"","date":1574294400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574294400,"objectID":"50b3d9dd4429b0077bb39fdbd11c05cb","permalink":"https://mea-lab.github.io/publication/2019a/","publishdate":"1969-12-31T19:33:39-05:00","relpermalink":"/publication/2019a/","section":"publication","summary":" **Importance**\nSurgeons have sought to optimize outcomes of smile reanimation surgery by combining inputs from nerve-to-masseter and cross-face nerve grafts. An objective assessment tool could help surgeons evaluate outcomes to determine the optimal neural sources for smile reanimation. **Objective**\nTo evaluate the use of a novel video time-stamping method and standard outcome measurement tools to assess outcomes of facial reanimation surgery using various innervation strategies. **Design, Setting, and Participants**\nCohort study assessing the outcomes of dually innervated gracilis free muscle transfers vs single-source innervated gracilis transfer performed at a tertiary care facial nerve center between 2007 and 2017 using a novel, video time-stamping spontaneity assessment method. The statistical analyses were performed in 2018. **Interventions**\nDually innervated gracilis free muscle transfers or single-source innervated gracilis transfer. **Main Outcomes and Measures**\nSpontaneous smiling was assessed by clinicians and quantified using blinded time-stamped video recordings of smiling elicited while viewing humorous video clips. **Results**\nThis retrospective cohort study included 25 patients (12 men and 13 women; median [range] age, 38.4 [29.3-46.0] years) treated with dually innervated gracilis free functional muscle graft for unilateral facial palsy between 2007 and 2017. Smile spontaneity assessment was performed in 17 patients and was compared with assessment performed in 24 patients treated with single-source innervated gracilis transfer (ie, nerve-to-masseter–driven or cross-face nerve graft–driven gracilis [n = 13]) (demographic data not available for NTM and CFNG cohorts). The use of time-stamped video assessment revealed that spontaneous synchronous oral commissure movement in a median percentage of smiles was 33% in patients with dually innervated gracilis (interquartile range [IQR], 0%-71%), 20% of smiles in patients with nerve-to-masseter–driven gracilis (IQR, 0%-50%), and 75% of smiles in patients with cross-face nerve graft–driven gracilis (IQR, 0%-100%). Clinicians graded smile spontaneity in dually innervated cases as absent in 40% (n = 6 of 15), trace in 33% (n = 5 of 15) and present in 27% (n = 4 of 15). No association was demonstrated between clinician-reported spontaneity and objectively measured synchronicity. **Conclusions and Relevance**\nDually innervated gracilis free muscle transfers may improve smile spontaneity compared with masseteric nerve–driven transfers but not to the level of cross-face nerve graft–driven gracilis transfers. Quantifying spontaneity is notoriously difficult, and most authors rely on clinical assessment. Our results suggest that clinicians may rate presence of spontaneity higher than objective measures, highlighting the importance of standardized assessment techniques. ","tags":["Facial Palsy"],"title":"Spontaneity Assessment in Dually Innervated Gracilis Smile Reanimation Surgery","type":"publication"},{"authors":["Jacqueline J. Greene","Joana Tavares","Diego L. Guarin","Tessa A. Hadlock"],"categories":["Facial Analysis"],"content":"","date":1567296000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567296000,"objectID":"47dd39c78fda4421b2a17261d8201f66","permalink":"https://mea-lab.github.io/publication/2019c/","publishdate":"1969-12-31T19:33:39-05:00","relpermalink":"/publication/2019c/","section":"publication","summary":" **Importance**\nQuantitative assessment of facial function is difficult, and historic grading scales such as House-Brackmann have well-recognized limitations. The electronic, clinician-graded facial function scale (eFACE) allows rapid regional analysis of static, dynamic, and synkinetic facial function in patients with unilateral facial palsy within the course of a clinical encounter, but it relies on clinician assessment. A newly developed, machine-learning algorithm (Emotrics) provides automated, objective facial measurements but lacks clinical input (ie, recognizing laterality of facial palsy or synkinesis). **Objectives**\nTo compare the sensitivity of a clinician-based tool (eFACE) to a well-established intervention for facial palsy (eyelid weight placement) with an automated facial-measurement algorithm (Emotrics). **Design, setting, and participants**\nA retrospective review was conducted of the most recent 53 patients with unilateral facial palsy who received an eyelid weight at the Massachusetts Eye and Ear Infirmary Facial Nerve Center from 2014 to 2017. Preoperative and postoperative photographs were deidentified and randomized. The entire cohort was analyzed by 3 clinicians, as well as by the Emotrics program. **Main outcomes and measures**\neFACE scores of the palpebral fissure at rest (0, wide; 100, balanced; 200, narrow), with gentle eyelid closure (0, incomplete; 100, complete), and with forceful eyelid closure (0, incomplete; 100, complete) before and after eyelid weight placement were compared with palpebral fissure measurements by Emotrics. **Results**\nOf the 53 participants, 33 were women, and mean (SD) age was 44.7 (18) years. The mean (SD) eFACE scores and Emotrics measurements (in millimeters) before vs after eyelid weight placement of the palpebral fissure at rest (eFACE, 84.3 [15.9] vs 109.7 [21.4]; Emotrics, 10.3 [2.2] vs 9.1 [1.8]), with gentle eyelid closure (eFACE, 65.9 [28.0] vs 92.1 [15.4]; Emotrics, 4.4 [2.7] vs 1.3 [2.0]), and with forceful eyelid closure (eFACE, 75.1 [28.6] vs 97.0 [10.7]; Emotrics, 3.0 [3.1] vs 0.5 [1.3]) all significantly improved. Subgroup analysis of patients with expected recovery (eg, Bell palsy) (n = 40) demonstrated significant development of ocular synkinesis on eFACE (83.9 [22.7] vs 98.9 [4.4]) after weight placement, which could also explain the improvement in eyelid function. The scores of patients with no expected recovery (n = 13) improved in both eFACE and Emotrics analysis following eyelid weight placement, though results did not reach significance, likely limited by the small subgroup size. **Conclusions and relevance**\nThe eFACE tool agrees well with automated, objective facial measurements using a machine-learning based algorithm such as Emotrics. The eFACE tool is sensitive to spontaneous recovery and surgical intervention, and may be used for rapid regional facial function assessment from a clinician's perspective following recovery and/or surgical intervention.;h ","tags":["Facial Analysis","Facial Palsy"],"title":"Clinician and Automated Assessments of Facial Function Following Eyelid Weight Placement","type":"publication"},{"authors":["Joseph R. Dusseldorp","Diego L. Guarin","Martinus M. van Veen","Nate Jowett","Tessa A. Hadlock"],"categories":["Facial Analysis"],"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"b91143a0df22daa3ca8e1a6011637a92","permalink":"https://mea-lab.github.io/publication/2019b/","publishdate":"1969-12-31T19:33:39-05:00","relpermalink":"/publication/2019b/","section":"publication","summary":" **Background**\nTools to quantify layperson assessments of facial palsy are lacking. In this study, artificial intelligence was applied to develop a proxy for layperson assessments, and compare sensitivity to existing outcome measures. **Methods**\nArtificially intelligent emotion detection software was used to develop the emotionality quotient. The emotionality quotient was defined as the percentage probability of perceived joy over the percentage probability of perceived negative emotions during smiling, as predicted by the software. The emotionality quotient was used to analyze the emotionality of voluntary smiles of normal subjects and unilateral facial palsy patients before and after smile reanimation. The emotionality quotient was compared to oral commissure excursion and layperson assessments of facial palsy patients. **Results**\nIn voluntary smiles of 10 normal subjects, 100 percent joy and no negative emotion was detected (interquartile ranges, 0/1). Median preoperative emotionality quotient of 30 facial palsy patients was 15/-60 (interquartile range, 73/62). Postoperatively, median emotionality quotient was 84/0 (interquartile range, 28/5). In 134 smile reanimation patients, no correlation was found between postoperative oral commissure excursion and emotionality quotient score. However, in 61 preoperative patients, a moderate correlation was found between layperson-assessed disfigurement and negative emotion perception (correlation coefficient, 0.516; p ","tags":["Facial Analysis","Facial Palsy","Computer-based Diagnosis"],"title":"In the eye of the beholder: Changes in perceived emotion expression after smile reanimation","type":"publication"},{"authors":["Diego L. Guarin","Joseph R. Dusseldorp","Tessa A. Hadlock","Nate Jowett"],"categories":["Facial Analysis"],"content":"","date":1531958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1531958400,"objectID":"8912e13d1b42bc7584e758ece22c1f82","permalink":"https://mea-lab.github.io/publication/2018a/","publishdate":"1969-12-31T19:33:38-05:00","relpermalink":"/publication/2018a/","section":"publication","summary":"","tags":["Facial Analysis","Facial Palsy"],"title":"A machine learning approach for automated facial measurements in facial palsy","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f1d044c0738ab9f19347f15c290a71a1","permalink":"https://mea-lab.github.io/research/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/research/","section":"","summary":"","tags":null,"title":"","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c1d17ff2b20dca0ad6653a3161942b64","permalink":"https://mea-lab.github.io/people/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/people/","section":"","summary":"","tags":null,"title":"People","type":"widget_page"}]